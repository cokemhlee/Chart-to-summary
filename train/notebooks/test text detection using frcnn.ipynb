{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!git clone https://github.com/kbardool/keras-frcnn.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cp '/kaggle/input/trainbbox/png/*' '/kaggle/working/keras-frcnn/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls '/kaggle/working/keras-frcnn/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !python keras-frcnn/keras_frcnn/simple_parser.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cp -r /kaggle/input/trainbbox/ /kaggle/working/\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !git clone https://github.com/you359/Keras-FasterRCNN.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/figureqabbox2/trainbbox/train_boundingbox.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(\"Unnamed: 0\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"x1\"] = df[\"x\"].astype('int')\ndf[\"y1\"] = df[\"y\"].astype('int')\ndf[\"x2\"] = (df[\"x\"] + df[\"w\"]).astype('int')\ndf[\"y2\"] = (df[\"y\"] + df[\"h\"]).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop([\"x\", \"y\", \"w\", \"h\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[[\"image\",\"x1\",\"y1\",\"x2\",\"y2\",\"class\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"image\"][1:23] = \"0.png\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"image\"] = \"/kaggle/input/trainbbox/png/\" + df[\"image\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('annotate.txt', header=None, index=None, sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install keras==2.1.6\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir out/ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_th_dim_ordering_th_kernels_notop.h5\n# https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.listdir('/kaggle/working/keras-frcnn/test_frcnn.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile /kaggle/working/keras-frcnn/test_frcnn.py\nfrom __future__ import division\nimport os\nimport cv2\nimport numpy as np\nimport sys\nimport pickle\nfrom optparse import OptionParser\nimport time\nfrom keras_frcnn import config\nfrom keras import backend as K\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras_frcnn import roi_helpers\n\nsys.setrecursionlimit(40000)\n\nparser = OptionParser()\n\nparser.add_option(\"-p\", \"--path\", dest=\"test_path\", help=\"Path to test data.\")\nparser.add_option(\"-n\", \"--num_rois\", type=\"int\", dest=\"num_rois\",\n\t\t\t\thelp=\"Number of ROIs per iteration. Higher means more memory use.\", default=32)\nparser.add_option(\"--config_filename\", dest=\"config_filename\", help=\n\t\t\t\t\"Location to read the metadata related to the training (generated when training).\",\n\t\t\t\tdefault=\"config.pickle\")\nparser.add_option(\"--network\", dest=\"network\", help=\"Base network to use. Supports vgg or resnet50.\", default='resnet50')\n\n(options, args) = parser.parse_args()\n\nif not options.test_path:   # if filename is not given\n\tparser.error('Error: path to test data must be specified. Pass --path to command line')\n\n\nconfig_output_filename = options.config_filename\n\nwith open(config_output_filename, 'rb') as f_in:\n\tC = pickle.load(f_in)\n\nif C.network == 'resnet50':\n\timport keras_frcnn.resnet as nn\nelif C.network == 'vgg':\n\timport keras_frcnn.vgg as nn\n\n# turn off any data augmentation at test time\nC.use_horizontal_flips = False\nC.use_vertical_flips = False\nC.rot_90 = False\n\nimg_path = options.test_path\n\ndef format_img_size(img, C):\n\t\"\"\" formats the image size based on config \"\"\"\n\timg_min_side = float(C.im_size)\n\t(height,width,_) = img.shape\n\t\t\n\tif width <= height:\n\t\tratio = img_min_side/width\n\t\tnew_height = int(ratio * height)\n\t\tnew_width = int(img_min_side)\n\telse:\n\t\tratio = img_min_side/height\n\t\tnew_width = int(ratio * width)\n\t\tnew_height = int(img_min_side)\n\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n\treturn img, ratio\t\n\ndef format_img_channels(img, C):\n\t\"\"\" formats the image channels based on config \"\"\"\n\timg = img[:, :, (2, 1, 0)]\n\timg = img.astype(np.float32)\n\timg[:, :, 0] -= C.img_channel_mean[0]\n\timg[:, :, 1] -= C.img_channel_mean[1]\n\timg[:, :, 2] -= C.img_channel_mean[2]\n\timg /= C.img_scaling_factor\n\timg = np.transpose(img, (2, 0, 1))\n\timg = np.expand_dims(img, axis=0)\n\treturn img\n\ndef format_img(img, C):\n\t\"\"\" formats an image for model prediction based on config \"\"\"\n\timg, ratio = format_img_size(img, C)\n\timg = format_img_channels(img, C)\n\treturn img, ratio\n\n# Method to transform the coordinates of the bounding box to its original size\ndef get_real_coordinates(ratio, x1, y1, x2, y2):\n\n\treal_x1 = int(round(x1 // ratio))\n\treal_y1 = int(round(y1 // ratio))\n\treal_x2 = int(round(x2 // ratio))\n\treal_y2 = int(round(y2 // ratio))\n\n\treturn (real_x1, real_y1, real_x2 ,real_y2)\n\nclass_mapping = C.class_mapping\n\nif 'bg' not in class_mapping:\n\tclass_mapping['bg'] = len(class_mapping)\n\nclass_mapping = {v: k for k, v in class_mapping.items()}\nprint(class_mapping)\nclass_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}\nC.num_rois = int(options.num_rois)\n\nif C.network == 'resnet50':\n\tnum_features = 1024\nelif C.network == 'vgg':\n\tnum_features = 512\n\nif K.image_dim_ordering() == 'th':\n\tinput_shape_img = (3, None, None)\n\tinput_shape_features = (num_features, None, None)\nelse:\n\tinput_shape_img = (None, None, 3)\n\tinput_shape_features = (None, None, num_features)\n\n\nimg_input = Input(shape=input_shape_img)\nroi_input = Input(shape=(C.num_rois, 4))\nfeature_map_input = Input(shape=input_shape_features)\n\n# define the base network (resnet here, can be VGG, Inception, etc)\nshared_layers = nn.nn_base(img_input, trainable=True)\n\n# define the RPN, built on the base layers\nnum_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\nrpn_layers = nn.rpn(shared_layers, num_anchors)\n\nclassifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)\n\nmodel_rpn = Model(img_input, rpn_layers)\nmodel_classifier_only = Model([feature_map_input, roi_input], classifier)\n\nmodel_classifier = Model([feature_map_input, roi_input], classifier)\n\nprint('Loading weights from {}'.format(C.model_path))\nmodel_rpn.load_weights(C.model_path, by_name=True)\nmodel_classifier.load_weights(C.model_path, by_name=True)\n\nmodel_rpn.compile(optimizer='sgd', loss='mse')\nmodel_classifier.compile(optimizer='sgd', loss='mse')\n\nall_imgs = []\n\nclasses = {}\n\nbbox_threshold = 0.8\n\nvisualise = True\n\nfor idx, img_name in enumerate(sorted(os.listdir(img_path))):\n\tif not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n\t\tcontinue\n\tprint(img_name)\n\tst = time.time()\n\tfilepath = os.path.join(img_path,img_name)\n\n\timg = cv2.imread(filepath)\n\n\tX, ratio = format_img(img, C)\n\n\tif K.image_dim_ordering() == 'tf':\n\t\tX = np.transpose(X, (0, 2, 3, 1))\n\n\t# get the feature maps and output from the RPN\n\t[Y1, Y2, F] = model_rpn.predict(X)\n\t\n\n\tR = roi_helpers.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), overlap_thresh=0.7)\n\n\t# convert from (x1,y1,x2,y2) to (x,y,w,h)\n\tR[:, 2] -= R[:, 0]\n\tR[:, 3] -= R[:, 1]\n\n\t# apply the spatial pyramid pooling to the proposed regions\n\tbboxes = {}\n\tprobs = {}\n\n\tfor jk in range(R.shape[0]//C.num_rois + 1):\n\t\tROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n\t\tif ROIs.shape[1] == 0:\n\t\t\tbreak\n\n\t\tif jk == R.shape[0]//C.num_rois:\n\t\t\t#pad R\n\t\t\tcurr_shape = ROIs.shape\n\t\t\ttarget_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n\t\t\tROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n\t\t\tROIs_padded[:, :curr_shape[1], :] = ROIs\n\t\t\tROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n\t\t\tROIs = ROIs_padded\n\n\t\t[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n\n\t\tfor ii in range(P_cls.shape[1]):\n\n\t\t\tif np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n\t\t\t\tcontinue\n\n\t\t\tcls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n\n\t\t\tif cls_name not in bboxes:\n\t\t\t\tbboxes[cls_name] = []\n\t\t\t\tprobs[cls_name] = []\n\n\t\t\t(x, y, w, h) = ROIs[0, ii, :]\n\n\t\t\tcls_num = np.argmax(P_cls[0, ii, :])\n\t\t\ttry:\n\t\t\t\t(tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n\t\t\t\ttx /= C.classifier_regr_std[0]\n\t\t\t\tty /= C.classifier_regr_std[1]\n\t\t\t\ttw /= C.classifier_regr_std[2]\n\t\t\t\tth /= C.classifier_regr_std[3]\n\t\t\t\tx, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tbboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n\t\t\tprobs[cls_name].append(np.max(P_cls[0, ii, :]))\n\n\tall_dets = []\n\n\tfor key in bboxes:\n\t\tbbox = np.array(bboxes[key])\n\n\t\tnew_boxes, new_probs = roi_helpers.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n\t\tfor jk in range(new_boxes.shape[0]):\n\t\t\t(x1, y1, x2, y2) = new_boxes[jk,:]\n\n\t\t\t(real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n\n\t\t\tcv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),2)\n\n\t\t\ttextLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n\t\t\tall_dets.append((key,100*new_probs[jk]))\n\n\t\t\t(retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n\t\t\ttextOrg = (real_x1, real_y1-0)\n\n\t\t\tcv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 2)\n\t\t\tcv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n\t\t\tcv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n\n\tprint('Elapsed time = {}'.format(time.time() - st))\n\tprint(all_dets)\n\tcv2.imwrite(\"/kaggle/working/out/{}.png\".format(idx), img)\n# \tcv2.waitKey(0)\n\t# cv2.imwrite('./results_imgs/{}.png'.format(idx),img)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir inimg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp /kaggle/input/figureqabbox2/validation1bbox/png/1.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/2.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/3.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/4.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/5.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/6.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/7.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/8.png /kaggle/working/inimg/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp /kaggle/input/figureqabbox2/validation1bbox/png/11.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/12.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/13.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/14.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/15.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/16.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/17.png /kaggle/working/inimg/\n!cp /kaggle/input/figureqabbox2/validation1bbox/png/18.png /kaggle/working/inimg/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp /kaggle/input/text-detection-using-frcnn/config.pickle /kaggle/working/\n!cp /kaggle/input/text-detection-using-frcnn/keras-frcnn.zip /kaggle/working/\n!cp /kaggle/input/text-detection-using-frcnn/model_frcnn.hdf5 /kaggle/working/\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python keras-frcnn/test_frcnn.py -p /kaggle/working/inimg/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 as cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 20]\n# plt.figure\nfor i in range(1,9):\n    print(i)\n    img = cv2.imread(\"/kaggle/working/out/{}.png\".format(i))\n    plt.imshow(img)\n    plt.show()\nfor i in range(11,19):\n    print(i)\n    if i == 16 or i == 17 or i==18 :\n        continue\n    img = cv2.imread(\"/kaggle/working/out/{}.png\".format(i))\n    plt.imshow(img)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get install zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!zip -r keras-frcnn.zip keras-frcnn/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -r keras-frcnn/  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm annotate.txt  resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp  ../input/text-detection-using-frcnn/config.pickle /kaggle/working/\n!cp  ../input/text-detection-using-frcnn/model_frcnn.hdf5 /kaggle/working/\n# !cp  ../input/text-detection-using-frcnn/config.pickle /kaggle/working/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install keras==2.1.6\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import importlib  \n# keras_frcnn = importlib.import_module(\"keras-frcnn.keras_frcnn\")\n# # from keras_frcnn.keras_frcnn import roi_helpers\n# # roi_helpers = importlib.import_module(\"keras-frcnn.keras_frcnn.roi_helpers\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"keras-frcnn/keras_frcnn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %load keras-frcnn/keras_frcnn/config.py\nfrom keras import backend as K\nimport math\n\nclass Config:\n\n\tdef __init__(self):\n\n\t\tself.verbose = True\n\n\t\tself.network = 'resnet50'\n\n\t\t# setting for data augmentation\n\t\tself.use_horizontal_flips = False\n\t\tself.use_vertical_flips = False\n\t\tself.rot_90 = False\n\n\t\t# anchor box scales\n\t\tself.anchor_box_scales = [128, 256, 512]\n\n\t\t# anchor box ratios\n\t\tself.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n\n\t\t# size to resize the smallest side of the image\n\t\tself.im_size = 600\n\n\t\t# image channel-wise mean to subtract\n\t\tself.img_channel_mean = [103.939, 116.779, 123.68]\n\t\tself.img_scaling_factor = 1.0\n\n\t\t# number of ROIs at once\n\t\tself.num_rois = 4\n\n\t\t# stride at the RPN (this depends on the network configuration)\n\t\tself.rpn_stride = 16\n\n\t\tself.balanced_classes = False\n\n\t\t# scaling the stdev\n\t\tself.std_scaling = 4.0\n\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n\n\t\t# overlaps for RPN\n\t\tself.rpn_min_overlap = 0.3\n\t\tself.rpn_max_overlap = 0.7\n\n\t\t# overlaps for classifier ROIs\n\t\tself.classifier_min_overlap = 0.1\n\t\tself.classifier_max_overlap = 0.5\n\n\t\t# placeholder for the class mapping, automatically generated by the parser\n\t\tself.class_mapping = None\n\n\t\t#location of pretrained weights for the base network \n\t\t# weight files can be found at:\n\t\t# https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_th_dim_ordering_th_kernels_notop.h5\n\t\t# https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n\t\tself.model_path = 'model_frcnn.vgg.hdf5'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %load keras-frcnn/keras_frcnn/data_augment.py\nimport cv2\nimport numpy as np\nimport copy\n\n\ndef augment(img_data, config, augment=True):\n\tassert 'filepath' in img_data\n\tassert 'bboxes' in img_data\n\tassert 'width' in img_data\n\tassert 'height' in img_data\n\n\timg_data_aug = copy.deepcopy(img_data)\n\n\timg = cv2.imread(img_data_aug['filepath'])\n\n\tif augment:\n\t\trows, cols = img.shape[:2]\n\n\t\tif config.use_horizontal_flips and np.random.randint(0, 2) == 0:\n\t\t\timg = cv2.flip(img, 1)\n\t\t\tfor bbox in img_data_aug['bboxes']:\n\t\t\t\tx1 = bbox['x1']\n\t\t\t\tx2 = bbox['x2']\n\t\t\t\tbbox['x2'] = cols - x1\n\t\t\t\tbbox['x1'] = cols - x2\n\n\t\tif config.use_vertical_flips and np.random.randint(0, 2) == 0:\n\t\t\timg = cv2.flip(img, 0)\n\t\t\tfor bbox in img_data_aug['bboxes']:\n\t\t\t\ty1 = bbox['y1']\n\t\t\t\ty2 = bbox['y2']\n\t\t\t\tbbox['y2'] = rows - y1\n\t\t\t\tbbox['y1'] = rows - y2\n\n\t\tif config.rot_90:\n\t\t\tangle = np.random.choice([0,90,180,270],1)[0]\n\t\t\tif angle == 270:\n\t\t\t\timg = np.transpose(img, (1,0,2))\n\t\t\t\timg = cv2.flip(img, 0)\n\t\t\telif angle == 180:\n\t\t\t\timg = cv2.flip(img, -1)\n\t\t\telif angle == 90:\n\t\t\t\timg = np.transpose(img, (1,0,2))\n\t\t\t\timg = cv2.flip(img, 1)\n\t\t\telif angle == 0:\n\t\t\t\tpass\n\n\t\t\tfor bbox in img_data_aug['bboxes']:\n\t\t\t\tx1 = bbox['x1']\n\t\t\t\tx2 = bbox['x2']\n\t\t\t\ty1 = bbox['y1']\n\t\t\t\ty2 = bbox['y2']\n\t\t\t\tif angle == 270:\n\t\t\t\t\tbbox['x1'] = y1\n\t\t\t\t\tbbox['x2'] = y2\n\t\t\t\t\tbbox['y1'] = cols - x2\n\t\t\t\t\tbbox['y2'] = cols - x1\n\t\t\t\telif angle == 180:\n\t\t\t\t\tbbox['x2'] = cols - x1\n\t\t\t\t\tbbox['x1'] = cols - x2\n\t\t\t\t\tbbox['y2'] = rows - y1\n\t\t\t\t\tbbox['y1'] = rows - y2\n\t\t\t\telif angle == 90:\n\t\t\t\t\tbbox['x1'] = rows - y2\n\t\t\t\t\tbbox['x2'] = rows - y1\n\t\t\t\t\tbbox['y1'] = x1\n\t\t\t\t\tbbox['y2'] = x2        \n\t\t\t\telif angle == 0:\n\t\t\t\t\tpass\n\n\timg_data_aug['width'] = img.shape[1]\n\timg_data_aug['height'] = img.shape[0]\n\treturn img_data_aug, img\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %load keras-frcnn/keras_frcnn/data_generators.py\nfrom __future__ import absolute_import\nimport numpy as np\nimport cv2\nimport random\nimport copy\n# from . import data_augment\nimport threading\nimport itertools\n\n\ndef union(au, bu, area_intersection):\n\tarea_a = (au[2] - au[0]) * (au[3] - au[1])\n\tarea_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n\tarea_union = area_a + area_b - area_intersection\n\treturn area_union\n\n\ndef intersection(ai, bi):\n\tx = max(ai[0], bi[0])\n\ty = max(ai[1], bi[1])\n\tw = min(ai[2], bi[2]) - x\n\th = min(ai[3], bi[3]) - y\n\tif w < 0 or h < 0:\n\t\treturn 0\n\treturn w*h\n\n\ndef iou(a, b):\n\t# a and b should be (x1,y1,x2,y2)\n\n\tif a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n\t\treturn 0.0\n\n\tarea_i = intersection(a, b)\n\tarea_u = union(a, b, area_i)\n\n\treturn float(area_i) / float(area_u + 1e-6)\n\n\ndef get_new_img_size(width, height, img_min_side=600):\n\tif width <= height:\n\t\tf = float(img_min_side) / width\n\t\tresized_height = int(f * height)\n\t\tresized_width = img_min_side\n\telse:\n\t\tf = float(img_min_side) / height\n\t\tresized_width = int(f * width)\n\t\tresized_height = img_min_side\n\n\treturn resized_width, resized_height\n\n\nclass SampleSelector:\n\tdef __init__(self, class_count):\n\t\t# ignore classes that have zero samples\n\t\tself.classes = [b for b in class_count.keys() if class_count[b] > 0]\n\t\tself.class_cycle = itertools.cycle(self.classes)\n\t\tself.curr_class = next(self.class_cycle)\n\n\tdef skip_sample_for_balanced_class(self, img_data):\n\n\t\tclass_in_img = False\n\n\t\tfor bbox in img_data['bboxes']:\n\n\t\t\tcls_name = bbox['class']\n\n\t\t\tif cls_name == self.curr_class:\n\t\t\t\tclass_in_img = True\n\t\t\t\tself.curr_class = next(self.class_cycle)\n\t\t\t\tbreak\n\n\t\tif class_in_img:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n\ndef calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n\n\tdownscale = float(C.rpn_stride)\n\tanchor_sizes = C.anchor_box_scales\n\tanchor_ratios = C.anchor_box_ratios\n\tnum_anchors = len(anchor_sizes) * len(anchor_ratios)\t\n\n\t# calculate the output map size based on the network architecture\n\n\t(output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n\n\tn_anchratios = len(anchor_ratios)\n\t\n\t# initialise empty output objectives\n\ty_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n\ty_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n\ty_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n\n\tnum_bboxes = len(img_data['bboxes'])\n\n\tnum_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n\tbest_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)\n\tbest_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n\tbest_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n\tbest_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n\n\t# get the GT box coordinates, and resize to account for image resizing\n\tgta = np.zeros((num_bboxes, 4))\n\tfor bbox_num, bbox in enumerate(img_data['bboxes']):\n\t\t# get the GT box coordinates, and resize to account for image resizing\n\t\tgta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n\t\tgta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n\t\tgta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n\t\tgta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n\t\n\t# rpn ground truth\n\n\tfor anchor_size_idx in range(len(anchor_sizes)):\n\t\tfor anchor_ratio_idx in range(n_anchratios):\n\t\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n\t\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\t\n\t\t\t\n\t\t\tfor ix in range(output_width):\t\t\t\t\t\n\t\t\t\t# x-coordinates of the current anchor box\t\n\t\t\t\tx1_anc = downscale * (ix + 0.5) - anchor_x / 2\n\t\t\t\tx2_anc = downscale * (ix + 0.5) + anchor_x / 2\t\n\t\t\t\t\n\t\t\t\t# ignore boxes that go across image boundaries\t\t\t\t\t\n\t\t\t\tif x1_anc < 0 or x2_anc > resized_width:\n\t\t\t\t\tcontinue\n\t\t\t\t\t\n\t\t\t\tfor jy in range(output_height):\n\n\t\t\t\t\t# y-coordinates of the current anchor box\n\t\t\t\t\ty1_anc = downscale * (jy + 0.5) - anchor_y / 2\n\t\t\t\t\ty2_anc = downscale * (jy + 0.5) + anchor_y / 2\n\n\t\t\t\t\t# ignore boxes that go across image boundaries\n\t\t\t\t\tif y1_anc < 0 or y2_anc > resized_height:\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t# bbox_type indicates whether an anchor should be a target \n\t\t\t\t\tbbox_type = 'neg'\n\n\t\t\t\t\t# this is the best IOU for the (x,y) coord and the current anchor\n\t\t\t\t\t# note that this is different from the best IOU for a GT bbox\n\t\t\t\t\tbest_iou_for_loc = 0.0\n\n\t\t\t\t\tfor bbox_num in range(num_bboxes):\n\t\t\t\t\t\t\n\t\t\t\t\t\t# get IOU of the current GT box and the current anchor box\n\t\t\t\t\t\tcurr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])\n\t\t\t\t\t\t# calculate the regression targets if they will be needed\n\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n\t\t\t\t\t\t\tcx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n\t\t\t\t\t\t\tcy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n\t\t\t\t\t\t\tcxa = (x1_anc + x2_anc)/2.0\n\t\t\t\t\t\t\tcya = (y1_anc + y2_anc)/2.0\n\n\t\t\t\t\t\t\ttx = (cx - cxa) / (x2_anc - x1_anc)\n\t\t\t\t\t\t\tty = (cy - cya) / (y2_anc - y1_anc)\n\t\t\t\t\t\t\ttw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n\t\t\t\t\t\t\tth = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n\t\t\t\t\t\t\n\t\t\t\t\t\tif img_data['bboxes'][bbox_num]['class'] != 'bg':\n\n\t\t\t\t\t\t\t# all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n\t\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num]:\n\t\t\t\t\t\t\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n\t\t\t\t\t\t\t\tbest_iou_for_bbox[bbox_num] = curr_iou\n\t\t\t\t\t\t\t\tbest_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]\n\t\t\t\t\t\t\t\tbest_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]\n\n\t\t\t\t\t\t\t# we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)\n\t\t\t\t\t\t\tif curr_iou > C.rpn_max_overlap:\n\t\t\t\t\t\t\t\tbbox_type = 'pos'\n\t\t\t\t\t\t\t\tnum_anchors_for_bbox[bbox_num] += 1\n\t\t\t\t\t\t\t\t# we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n\t\t\t\t\t\t\t\tif curr_iou > best_iou_for_loc:\n\t\t\t\t\t\t\t\t\tbest_iou_for_loc = curr_iou\n\t\t\t\t\t\t\t\t\tbest_regr = (tx, ty, tw, th)\n\n\t\t\t\t\t\t\t# if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective\n\t\t\t\t\t\t\tif C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n\t\t\t\t\t\t\t\t# gray zone between neg and pos\n\t\t\t\t\t\t\t\tif bbox_type != 'pos':\n\t\t\t\t\t\t\t\t\tbbox_type = 'neutral'\n\n\t\t\t\t\t# turn on or off outputs depending on IOUs\n\t\t\t\t\tif bbox_type == 'neg':\n\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n\t\t\t\t\telif bbox_type == 'neutral':\n\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n\t\t\t\t\telif bbox_type == 'pos':\n\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n\t\t\t\t\t\tstart = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n\t\t\t\t\t\ty_rpn_regr[jy, ix, start:start+4] = best_regr\n\n\t# we ensure that every bbox has at least one positive RPN region\n\n\tfor idx in range(num_anchors_for_bbox.shape[0]):\n\t\tif num_anchors_for_bbox[idx] == 0:\n\t\t\t# no box with an IOU greater than zero ...\n\t\t\tif best_anchor_for_bbox[idx, 0] == -1:\n\t\t\t\tcontinue\n\t\t\ty_is_box_valid[\n\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n\t\t\ty_rpn_overlap[\n\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n\t\t\tstart = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n\t\t\ty_rpn_regr[\n\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n\n\ty_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n\ty_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n\n\ty_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n\ty_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n\n\ty_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n\ty_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n\n\tpos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n\tneg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n\n\tnum_pos = len(pos_locs[0])\n\n\t# one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative\n\t# regions. We also limit it to 256 regions.\n\tnum_regions = 256\n\n\tif len(pos_locs[0]) > num_regions/2:\n\t\tval_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)\n\t\ty_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n\t\tnum_pos = num_regions/2\n\n\tif len(neg_locs[0]) + num_pos > num_regions:\n\t\tval_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n\t\ty_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n\n\ty_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n\ty_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n\n\treturn np.copy(y_rpn_cls), np.copy(y_rpn_regr)\n\n\nclass threadsafe_iter:\n\t\"\"\"Takes an iterator/generator and makes it thread-safe by\n\tserializing call to the `next` method of given iterator/generator.\n\t\"\"\"\n\tdef __init__(self, it):\n\t\tself.it = it\n\t\tself.lock = threading.Lock()\n\n\tdef __iter__(self):\n\t\treturn self\n\n\tdef next(self):\n\t\twith self.lock:\n\t\t\treturn next(self.it)\t\t\n\n\t\ndef threadsafe_generator(f):\n\t\"\"\"A decorator that takes a generator function and makes it thread-safe.\n\t\"\"\"\n\tdef g(*a, **kw):\n\t\treturn threadsafe_iter(f(*a, **kw))\n\treturn g\n\ndef get_anchor_gt(all_img_data, class_count, C, img_length_calc_function, backend, mode='train'):\n\n\t# The following line is not useful with Python 3.5, it is kept for the legacy\n\t# all_img_data = sorted(all_img_data)\n\n\tsample_selector = SampleSelector(class_count)\n\n\twhile True:\n\t\tif mode == 'train':\n\t\t\tnp.random.shuffle(all_img_data)\n\n\t\tfor img_data in all_img_data:\n\t\t\ttry:\n\n\t\t\t\tif C.balanced_classes and sample_selector.skip_sample_for_balanced_class(img_data):\n\t\t\t\t\tcontinue\n\n\t\t\t\t# read in image, and optionally add augmentation\n\n\t\t\t\tif mode == 'train':\n\t\t\t\t\timg_data_aug, x_img = data_augment.augment(img_data, C, augment=True)\n\t\t\t\telse:\n\t\t\t\t\timg_data_aug, x_img = data_augment.augment(img_data, C, augment=False)\n\n\t\t\t\t(width, height) = (img_data_aug['width'], img_data_aug['height'])\n\t\t\t\t(rows, cols, _) = x_img.shape\n\n\t\t\t\tassert cols == width\n\t\t\t\tassert rows == height\n\n\t\t\t\t# get image dimensions for resizing\n\t\t\t\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n\n\t\t\t\t# resize the image so that smalles side is length = 600px\n\t\t\t\tx_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n\n\t\t\t\ttry:\n\t\t\t\t\ty_rpn_cls, y_rpn_regr = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n\t\t\t\texcept:\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Zero-center by mean pixel, and preprocess image\n\n\t\t\t\tx_img = x_img[:,:, (2, 1, 0)]  # BGR -> RGB\n\t\t\t\tx_img = x_img.astype(np.float32)\n\t\t\t\tx_img[:, :, 0] -= C.img_channel_mean[0]\n\t\t\t\tx_img[:, :, 1] -= C.img_channel_mean[1]\n\t\t\t\tx_img[:, :, 2] -= C.img_channel_mean[2]\n\t\t\t\tx_img /= C.img_scaling_factor\n\n\t\t\t\tx_img = np.transpose(x_img, (2, 0, 1))\n\t\t\t\tx_img = np.expand_dims(x_img, axis=0)\n\n\t\t\t\ty_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling\n\n\t\t\t\tif backend == 'tf':\n\t\t\t\t\tx_img = np.transpose(x_img, (0, 2, 3, 1))\n\t\t\t\t\ty_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))\n\t\t\t\t\ty_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))\n\n\t\t\t\tyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug\n\n\t\t\texcept Exception as e:\n\t\t\t\tprint(e)\n\t\t\t\tcontinue\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %load keras-frcnn/keras_frcnn/roi_helpers.py\nimport numpy as np\nimport pdb\nimport math\n# from . import data_generators\nimport copy\n\n\ndef calc_iou(R, img_data, C, class_mapping):\n\n\tbboxes = img_data['bboxes']\n\t(width, height) = (img_data['width'], img_data['height'])\n\t# get image dimensions for resizing\n\t(resized_width, resized_height) = data_generators.get_new_img_size(width, height, C.im_size)\n\n\tgta = np.zeros((len(bboxes), 4))\n\n\tfor bbox_num, bbox in enumerate(bboxes):\n\t\t# get the GT box coordinates, and resize to account for image resizing\n\t\tgta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/C.rpn_stride))\n\t\tgta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width))/C.rpn_stride))\n\t\tgta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height))/C.rpn_stride))\n\t\tgta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/C.rpn_stride))\n\n\tx_roi = []\n\ty_class_num = []\n\ty_class_regr_coords = []\n\ty_class_regr_label = []\n\tIoUs = [] # for debugging only\n\n\tfor ix in range(R.shape[0]):\n\t\t(x1, y1, x2, y2) = R[ix, :]\n\t\tx1 = int(round(x1))\n\t\ty1 = int(round(y1))\n\t\tx2 = int(round(x2))\n\t\ty2 = int(round(y2))\n\n\t\tbest_iou = 0.0\n\t\tbest_bbox = -1\n\t\tfor bbox_num in range(len(bboxes)):\n\t\t\tcurr_iou = data_generators.iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])\n\t\t\tif curr_iou > best_iou:\n\t\t\t\tbest_iou = curr_iou\n\t\t\t\tbest_bbox = bbox_num\n\n\t\tif best_iou < C.classifier_min_overlap:\n\t\t\t\tcontinue\n\t\telse:\n\t\t\tw = x2 - x1\n\t\t\th = y2 - y1\n\t\t\tx_roi.append([x1, y1, w, h])\n\t\t\tIoUs.append(best_iou)\n\n\t\t\tif C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:\n\t\t\t\t# hard negative example\n\t\t\t\tcls_name = 'bg'\n\t\t\telif C.classifier_max_overlap <= best_iou:\n\t\t\t\tcls_name = bboxes[best_bbox]['class']\n\t\t\t\tcxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n\t\t\t\tcyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n\n\t\t\t\tcx = x1 + w / 2.0\n\t\t\t\tcy = y1 + h / 2.0\n\n\t\t\t\ttx = (cxg - cx) / float(w)\n\t\t\t\tty = (cyg - cy) / float(h)\n\t\t\t\ttw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n\t\t\t\tth = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n\t\t\telse:\n\t\t\t\tprint('roi = {}'.format(best_iou))\n\t\t\t\traise RuntimeError\n\n\t\tclass_num = class_mapping[cls_name]\n\t\tclass_label = len(class_mapping) * [0]\n\t\tclass_label[class_num] = 1\n\t\ty_class_num.append(copy.deepcopy(class_label))\n\t\tcoords = [0] * 4 * (len(class_mapping) - 1)\n\t\tlabels = [0] * 4 * (len(class_mapping) - 1)\n\t\tif cls_name != 'bg':\n\t\t\tlabel_pos = 4 * class_num\n\t\t\tsx, sy, sw, sh = C.classifier_regr_std\n\t\t\tcoords[label_pos:4+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]\n\t\t\tlabels[label_pos:4+label_pos] = [1, 1, 1, 1]\n\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))\n\t\t\ty_class_regr_label.append(copy.deepcopy(labels))\n\t\telse:\n\t\t\ty_class_regr_coords.append(copy.deepcopy(coords))\n\t\t\ty_class_regr_label.append(copy.deepcopy(labels))\n\n\tif len(x_roi) == 0:\n\t\treturn None, None, None, None\n\n\tX = np.array(x_roi)\n\tY1 = np.array(y_class_num)\n\tY2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=1)\n\n\treturn np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs\n\ndef apply_regr(x, y, w, h, tx, ty, tw, th):\n\ttry:\n\t\tcx = x + w/2.\n\t\tcy = y + h/2.\n\t\tcx1 = tx * w + cx\n\t\tcy1 = ty * h + cy\n\t\tw1 = math.exp(tw) * w\n\t\th1 = math.exp(th) * h\n\t\tx1 = cx1 - w1/2.\n\t\ty1 = cy1 - h1/2.\n\t\tx1 = int(round(x1))\n\t\ty1 = int(round(y1))\n\t\tw1 = int(round(w1))\n\t\th1 = int(round(h1))\n\n\t\treturn x1, y1, w1, h1\n\n\texcept ValueError:\n\t\treturn x, y, w, h\n\texcept OverflowError:\n\t\treturn x, y, w, h\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn x, y, w, h\n\ndef apply_regr_np(X, T):\n\ttry:\n\t\tx = X[0, :, :]\n\t\ty = X[1, :, :]\n\t\tw = X[2, :, :]\n\t\th = X[3, :, :]\n\n\t\ttx = T[0, :, :]\n\t\tty = T[1, :, :]\n\t\ttw = T[2, :, :]\n\t\tth = T[3, :, :]\n\n\t\tcx = x + w/2.\n\t\tcy = y + h/2.\n\t\tcx1 = tx * w + cx\n\t\tcy1 = ty * h + cy\n\n\t\tw1 = np.exp(tw.astype(np.float64)) * w\n\t\th1 = np.exp(th.astype(np.float64)) * h\n\t\tx1 = cx1 - w1/2.\n\t\ty1 = cy1 - h1/2.\n\n\t\tx1 = np.round(x1)\n\t\ty1 = np.round(y1)\n\t\tw1 = np.round(w1)\n\t\th1 = np.round(h1)\n\t\treturn np.stack([x1, y1, w1, h1])\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn X\n\ndef non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n\t# code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n\t# if there are no boxes, return an empty list\n\tif len(boxes) == 0:\n\t\treturn []\n\n\t# grab the coordinates of the bounding boxes\n\tx1 = boxes[:, 0]\n\ty1 = boxes[:, 1]\n\tx2 = boxes[:, 2]\n\ty2 = boxes[:, 3]\n\n\tnp.testing.assert_array_less(x1, x2)\n\tnp.testing.assert_array_less(y1, y2)\n\n\t# if the bounding boxes integers, convert them to floats --\n\t# this is important since we'll be doing a bunch of divisions\n\tif boxes.dtype.kind == \"i\":\n\t\tboxes = boxes.astype(\"float\")\n\n\t# initialize the list of picked indexes\t\n\tpick = []\n\n\t# calculate the areas\n\tarea = (x2 - x1) * (y2 - y1)\n\n\t# sort the bounding boxes \n\tidxs = np.argsort(probs)\n\n\t# keep looping while some indexes still remain in the indexes\n\t# list\n\twhile len(idxs) > 0:\n\t\t# grab the last index in the indexes list and add the\n\t\t# index value to the list of picked indexes\n\t\tlast = len(idxs) - 1\n\t\ti = idxs[last]\n\t\tpick.append(i)\n\n\t\t# find the intersection\n\n\t\txx1_int = np.maximum(x1[i], x1[idxs[:last]])\n\t\tyy1_int = np.maximum(y1[i], y1[idxs[:last]])\n\t\txx2_int = np.minimum(x2[i], x2[idxs[:last]])\n\t\tyy2_int = np.minimum(y2[i], y2[idxs[:last]])\n\n\t\tww_int = np.maximum(0, xx2_int - xx1_int)\n\t\thh_int = np.maximum(0, yy2_int - yy1_int)\n\n\t\tarea_int = ww_int * hh_int\n\n\t\t# find the union\n\t\tarea_union = area[i] + area[idxs[:last]] - area_int\n\n\t\t# compute the ratio of overlap\n\t\toverlap = area_int/(area_union + 1e-6)\n\n\t\t# delete all indexes from the index list that have\n\t\tidxs = np.delete(idxs, np.concatenate(([last],\n\t\t\tnp.where(overlap > overlap_thresh)[0])))\n\n\t\tif len(pick) >= max_boxes:\n\t\t\tbreak\n\n\t# return only the bounding boxes that were picked using the integer data type\n\tboxes = boxes[pick].astype(\"int\")\n\tprobs = probs[pick]\n\treturn boxes, probs\n\nimport time\ndef rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=100000,overlap_thresh=0.9):\n\n\tregr_layer = regr_layer / C.std_scaling\n\n\tanchor_sizes = C.anchor_box_scales\n\tanchor_ratios = C.anchor_box_ratios\n\n\tassert rpn_layer.shape[0] == 1\n\n\tif dim_ordering == 'th':\n\t\t(rows,cols) = rpn_layer.shape[2:]\n\n\telif dim_ordering == 'tf':\n\t\t(rows, cols) = rpn_layer.shape[1:3]\n\n\tcurr_layer = 0\n\tif dim_ordering == 'tf':\n\t\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n\telif dim_ordering == 'th':\n\t\tA = np.zeros((4, rpn_layer.shape[2], rpn_layer.shape[3], rpn_layer.shape[1]))\n\n\tfor anchor_size in anchor_sizes:\n\t\tfor anchor_ratio in anchor_ratios:\n\n\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n\t\t\tif dim_ordering == 'th':\n\t\t\t\tregr = regr_layer[0, 4 * curr_layer:4 * curr_layer + 4, :, :]\n\t\t\telse:\n\t\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]\n\t\t\t\tregr = np.transpose(regr, (2, 0, 1))\n\n\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n\n\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2\n\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2\n\t\t\tA[2, :, :, curr_layer] = anchor_x\n\t\t\tA[3, :, :, curr_layer] = anchor_y\n\n\t\t\tif use_regr:\n\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n\n\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n\n\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n\n\t\t\tcurr_layer += 1\n\n\tall_boxes = np.reshape(A.transpose((0, 3, 1,2)), (4, -1)).transpose((1, 0))\n\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))\n\n\tx1 = all_boxes[:, 0]\n\ty1 = all_boxes[:, 1]\n\tx2 = all_boxes[:, 2]\n\ty2 = all_boxes[:, 3]\n\n\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n\n\tall_boxes = np.delete(all_boxes, idxs, 0)\n\tall_probs = np.delete(all_probs, idxs, 0)\n\n\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n\n\treturn result\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile keras-frcnn/test_frcnn.py\nfrom __future__ import division\nimport os\nimport cv2\nimport numpy as np\nimport sys\nimport pickle\n# from optparse import OptionParser\nimport time\n# from keras_frcnn import config\nfrom keras import backend as K\nfrom keras.layers import Input\nfrom keras.models import Model\n# from keras_frcnn import roi_helpers\n\nsys.setrecursionlimit(40000)\n\n# parser = OptionParser()\n\n# parser.add_option(\"-p\", \"--path\", dest=\"test_path\", help=\"Path to test data.\")\n# parser.add_option(\"-n\", \"--num_rois\", type=\"int\", dest=\"num_rois\",\n# \t\t\t\thelp=\"Number of ROIs per iteration. Higher means more memory use.\", default=32)\n# parser.add_option(\"--config_filename\", dest=\"config_filename\", help=\n# \t\t\t\t\"Location to read the metadata related to the training (generated when training).\",\n# \t\t\t\tdefault=\"config.pickle\")\n# parser.add_option(\"--network\", dest=\"network\", help=\"Base network to use. Supports vgg or resnet50.\", default='resnet50')\n\n# (options, args) = parser.parse_args()\n\n# if not options.test_path:   # if filename is not given\n# \tparser.error('Error: path to test data must be specified. Pass --path to command line')\n\n\nconfig_output_filename = \"config.pickle\"\n\nwith open(config_output_filename, 'rb') as f_in:\n\tC = pickle.load(f_in)\n\nif C.network == 'resnet50':\n\timport keras_frcnn.resnet as nn\nelif C.network == 'vgg':\n\timport keras_frcnn.vgg as nn\n\n# turn off any data augmentation at test time\nC.use_horizontal_flips = False\nC.use_vertical_flips = False\nC.rot_90 = False\n\nimg_path = \"/kaggle/input/figureqabbox2/validation1bbox/png/\"\n\ndef format_img_size(img, C):\n\t\"\"\" formats the image size based on config \"\"\"\n\timg_min_side = float(C.im_size)\n\t(height,width,_) = img.shape\n\t\t\n\tif width <= height:\n\t\tratio = img_min_side/width\n\t\tnew_height = int(ratio * height)\n\t\tnew_width = int(img_min_side)\n\telse:\n\t\tratio = img_min_side/height\n\t\tnew_width = int(ratio * width)\n\t\tnew_height = int(img_min_side)\n\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n\treturn img, ratio\t\n\ndef format_img_channels(img, C):\n\t\"\"\" formats the image channels based on config \"\"\"\n\timg = img[:, :, (2, 1, 0)]\n\timg = img.astype(np.float32)\n\timg[:, :, 0] -= C.img_channel_mean[0]\n\timg[:, :, 1] -= C.img_channel_mean[1]\n\timg[:, :, 2] -= C.img_channel_mean[2]\n\timg /= C.img_scaling_factor\n\timg = np.transpose(img, (2, 0, 1))\n\timg = np.expand_dims(img, axis=0)\n\treturn img\n\ndef format_img(img, C):\n\t\"\"\" formats an image for model prediction based on config \"\"\"\n\timg, ratio = format_img_size(img, C)\n\timg = format_img_channels(img, C)\n\treturn img, ratio\n\n# Method to transform the coordinates of the bounding box to its original size\ndef get_real_coordinates(ratio, x1, y1, x2, y2):\n\n\treal_x1 = int(round(x1 // ratio))\n\treal_y1 = int(round(y1 // ratio))\n\treal_x2 = int(round(x2 // ratio))\n\treal_y2 = int(round(y2 // ratio))\n\n\treturn (real_x1, real_y1, real_x2 ,real_y2)\n\nclass_mapping = C.class_mapping\n\nif 'bg' not in class_mapping:\n\tclass_mapping['bg'] = len(class_mapping)\n\nclass_mapping = {v: k for k, v in class_mapping.items()}\nprint(class_mapping)\nclass_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}\nC.num_rois = int(32)\n\nif C.network == 'resnet50':\n\tnum_features = 1024\nelif C.network == 'vgg':\n\tnum_features = 512\n\nif K.image_dim_ordering() == 'th':\n\tinput_shape_img = (3, None, None)\n\tinput_shape_features = (num_features, None, None)\nelse:\n\tinput_shape_img = (None, None, 3)\n\tinput_shape_features = (None, None, num_features)\n\n\nimg_input = Input(shape=input_shape_img)\nroi_input = Input(shape=(C.num_rois, 4))\nfeature_map_input = Input(shape=input_shape_features)\n\n# define the base network (resnet here, can be VGG, Inception, etc)\nshared_layers = nn.nn_base(img_input, trainable=True)\n\n# define the RPN, built on the base layers\nnum_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\nrpn_layers = nn.rpn(shared_layers, num_anchors)\n\nclassifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)\n\nmodel_rpn = Model(img_input, rpn_layers)\nmodel_classifier_only = Model([feature_map_input, roi_input], classifier)\n\nmodel_classifier = Model([feature_map_input, roi_input], classifier)\n\nprint('Loading weights from {}'.format(C.model_path))\nmodel_rpn.load_weights(C.model_path, by_name=True)\nmodel_classifier.load_weights(C.model_path, by_name=True)\n\nmodel_rpn.compile(optimizer='sgd', loss='mse')\nmodel_classifier.compile(optimizer='sgd', loss='mse')\n\nall_imgs = []\n\nclasses = {}\n\nbbox_threshold = 0.8\n\nvisualise = True\n\nfor idx, img_name in enumerate(sorted(os.listdir(img_path))):\n\tif not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n\t\tcontinue\n\tprint(img_name)\n\tst = time.time()\n\tfilepath = os.path.join(img_path,img_name)\n\n\timg = cv2.imread(filepath)\n\n\tX, ratio = format_img(img, C)\n\n\tif K.image_dim_ordering() == 'tf':\n\t\tX = np.transpose(X, (0, 2, 3, 1))\n\n\t# get the feature maps and output from the RPN\n\t[Y1, Y2, F] = model_rpn.predict(X)\n\t\n\n\tR = roi_helpers.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), overlap_thresh=0.7)\n\n\t# convert from (x1,y1,x2,y2) to (x,y,w,h)\n\tR[:, 2] -= R[:, 0]\n\tR[:, 3] -= R[:, 1]\n\n\t# apply the spatial pyramid pooling to the proposed regions\n\tbboxes = {}\n\tprobs = {}\n\n\tfor jk in range(R.shape[0]//C.num_rois + 1):\n\t\tROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n\t\tif ROIs.shape[1] == 0:\n\t\t\tbreak\n\n\t\tif jk == R.shape[0]//C.num_rois:\n\t\t\t#pad R\n\t\t\tcurr_shape = ROIs.shape\n\t\t\ttarget_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n\t\t\tROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n\t\t\tROIs_padded[:, :curr_shape[1], :] = ROIs\n\t\t\tROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n\t\t\tROIs = ROIs_padded\n\n\t\t[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n\n\t\tfor ii in range(P_cls.shape[1]):\n\n\t\t\tif np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n\t\t\t\tcontinue\n\n\t\t\tcls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n\n\t\t\tif cls_name not in bboxes:\n\t\t\t\tbboxes[cls_name] = []\n\t\t\t\tprobs[cls_name] = []\n\n\t\t\t(x, y, w, h) = ROIs[0, ii, :]\n\n\t\t\tcls_num = np.argmax(P_cls[0, ii, :])\n\t\t\ttry:\n\t\t\t\t(tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n\t\t\t\ttx /= C.classifier_regr_std[0]\n\t\t\t\tty /= C.classifier_regr_std[1]\n\t\t\t\ttw /= C.classifier_regr_std[2]\n\t\t\t\tth /= C.classifier_regr_std[3]\n\t\t\t\tx, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tbboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n\t\t\tprobs[cls_name].append(np.max(P_cls[0, ii, :]))\n\n\tall_dets = []\n\n\tfor key in bboxes:\n\t\tbbox = np.array(bboxes[key])\n\n\t\tnew_boxes, new_probs = roi_helpers.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n\t\tfor jk in range(new_boxes.shape[0]):\n\t\t\t(x1, y1, x2, y2) = new_boxes[jk,:]\n\n\t\t\t(real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n\n\t\t\tcv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),2)\n\n\t\t\ttextLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n\t\t\tall_dets.append((key,100*new_probs[jk]))\n\n\t\t\t(retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n\t\t\ttextOrg = (real_x1, real_y1-0)\n\n\t\t\tcv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 2)\n\t\t\tcv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n\t\t\tcv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n\n\tprint('Elapsed time = {}'.format(time.time() - st))\n\tprint(all_dets)\n\tplt.imshow(img)\n# \tcv2.imshow('img', img)\n\tplt.show()\n# \tcv2.waitKey(0)\n\t# cv2.imwrite('./results_imgs/{}.png'.format(idx),img)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python keras-frcnn/test_frcnn.py -p /kaggle/input/figureqabbox2/validation1bbox/png/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 as cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 20]\n# plt.figure\nfor i in range(1,9):\n    print(i)\n    img = cv2.imread(\"/kaggle/working/out/{}.png\".format(i))\n    plt.imshow(img)\n    plt.show()\nfor i in range(11,19):\n    print(i)\n    if i == 16 or i == 17 or i==18 :\n        continue\n    img = cv2.imread(\"/kaggle/working/out/{}.png\".format(i))\n    plt.imshow(img)\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}